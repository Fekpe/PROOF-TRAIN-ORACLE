### Dataset Evaluation Output

```
{
    "dataset_hash": "6c3477c891ce818aad78e837fc4af5536037fb5fb24e75f6b47223d9d6376057",
    "quality_score": 100,
    "metrics": {
        "rows": 4,
        "columns": 4,
        "missing_ratio": 0.062,
        "numeric_cols": 2,
        "categorical_cols": 2,
        "bias_metric": 0.375
    }
}
```

Generated by `backend/dataset_eval.py` (run from backend folder).
### Dataset Evaluation Output

```
{
    "dataset_hash": "6c3477c891ce818aad78e837fc4af5536037fb5fb24e75f6b47223d9d6376057",
    "quality_score": 100,
    "metrics": {
        "rows": 4,
        "columns": 4,
        "missing_ratio": 0.062,
        "numeric_cols": 2,
        "categorical_cols": 2,
        "bias_metric": 0.375
    }
}
```

Generated by `backend/dataset_eval.py` (run from backend folder).

## 2-minute demo script

Hi — this is a 2-minute demo of Proof Train Oracle. I'll walk you through the full pipeline.

1) Upload a CSV (we use `backend/sample_datasets/sample.csv`). The dataset evaluator computes rows/columns, missing data and a simple bias metric.

2) The trainer runs a small logistic regression on the cleaned dataset and produces a model proof: dataset hash, model hash, accuracy, and training time. The trainer writes `backend/output_metadata.json`.

3) The backend pushes the proof to the on-chain registry simulator (`blockchain/interactions.py`) which appends an entry in `blockchain/mock_chain_log.json` and returns a mock transaction hash.

4) The frontend dashboard (Vite + React) consumes mock JSON files under `frontend/mock/` (which we update after running the pipeline) and displays evaluated datasets and model proofs.

That's the entire flow: Evaluate → Train → Push → Display. The run script `backend/run_pipeline.py` automates these steps locally.

Run it locally:
```
cd backend
python run_pipeline.py
```

This will update `frontend/mock/models.json` and `frontend/mock/datasets.json` so the dashboard shows the latest results.
